{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, BatchNormalization, Add, Average, Concatenate, Lambda)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "import keras_tuner as kt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "           tiv        csfv         gmv         wmv\n",
      "0  1434.357361  219.565569  678.230161  535.878459\n",
      "1  1558.161428  261.968669  756.742506  538.817738\n",
      "2  1418.050690  242.123816  686.814910  488.740580\n",
      "3  1304.233543  206.305238  667.809720  429.723510\n",
      "4  1660.856147  344.783456  703.484560  611.222413\n",
      "\n",
      "Target:\n",
      "0    19.0\n",
      "1    21.0\n",
      "2    21.0\n",
      "3    15.0\n",
      "4    31.0\n",
      "Name: age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Brain.tsv\", sep='\\t')\n",
    "features = ['tiv', 'csfv', 'gmv', 'wmv']\n",
    "target = 'age'\n",
    "X = data[features].values\n",
    "y = data[target].values\n",
    "\n",
    "print(\"Features:\")\n",
    "print(data[features].head())\n",
    "\n",
    "print(\"\\nTarget:\")\n",
    "print(data[target].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>age</th>\n",
       "      <th>tiv</th>\n",
       "      <th>csfv</th>\n",
       "      <th>gmv</th>\n",
       "      <th>wmv</th>\n",
       "      <th>magnetic_field_strength</th>\n",
       "      <th>acquisition_setting</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.984000e+03</td>\n",
       "      <td>3984.000000</td>\n",
       "      <td>3984.000000</td>\n",
       "      <td>3984.000000</td>\n",
       "      <td>3984.000000</td>\n",
       "      <td>3984.000000</td>\n",
       "      <td>3984.000000</td>\n",
       "      <td>3984.000000</td>\n",
       "      <td>3984.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.542203e+11</td>\n",
       "      <td>24.922390</td>\n",
       "      <td>1450.235934</td>\n",
       "      <td>253.048198</td>\n",
       "      <td>685.158274</td>\n",
       "      <td>511.076201</td>\n",
       "      <td>2.869352</td>\n",
       "      <td>1.207078</td>\n",
       "      <td>17.887299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.605308e+11</td>\n",
       "      <td>14.287559</td>\n",
       "      <td>144.051540</td>\n",
       "      <td>62.063662</td>\n",
       "      <td>88.062662</td>\n",
       "      <td>62.508482</td>\n",
       "      <td>0.423022</td>\n",
       "      <td>0.494550</td>\n",
       "      <td>16.739054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000532e+11</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>762.709988</td>\n",
       "      <td>100.219412</td>\n",
       "      <td>260.969929</td>\n",
       "      <td>313.087018</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.299751e+11</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1348.497432</td>\n",
       "      <td>211.831925</td>\n",
       "      <td>629.440521</td>\n",
       "      <td>466.820755</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.485667e+11</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1444.412188</td>\n",
       "      <td>243.370296</td>\n",
       "      <td>684.537534</td>\n",
       "      <td>507.093032</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.823813e+11</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1548.161334</td>\n",
       "      <td>283.595180</td>\n",
       "      <td>742.164149</td>\n",
       "      <td>552.503305</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.998326e+11</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>2101.970447</td>\n",
       "      <td>624.906527</td>\n",
       "      <td>986.262878</td>\n",
       "      <td>808.156251</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       participant_id          age          tiv         csfv          gmv  \\\n",
       "count    3.984000e+03  3984.000000  3984.000000  3984.000000  3984.000000   \n",
       "mean     5.542203e+11    24.922390  1450.235934   253.048198   685.158274   \n",
       "std      2.605308e+11    14.287559   144.051540    62.063662    88.062662   \n",
       "min      1.000532e+11     5.900000   762.709988   100.219412   260.969929   \n",
       "25%      3.299751e+11    19.000000  1348.497432   211.831925   629.440521   \n",
       "50%      5.485667e+11    21.000000  1444.412188   243.370296   684.537534   \n",
       "75%      7.823813e+11    26.000000  1548.161334   283.595180   742.164149   \n",
       "max      9.998326e+11    88.000000  2101.970447   624.906527   986.262878   \n",
       "\n",
       "               wmv  magnetic_field_strength  acquisition_setting         site  \n",
       "count  3984.000000              3984.000000          3984.000000  3984.000000  \n",
       "mean    511.076201                 2.869352             1.207078    17.887299  \n",
       "std      62.508482                 0.423022             0.494550    16.739054  \n",
       "min     313.087018                 1.500000             1.000000     0.000000  \n",
       "25%     466.820755                 3.000000             1.000000     3.000000  \n",
       "50%     507.093032                 3.000000             1.000000    14.000000  \n",
       "75%     552.503305                 3.000000             1.000000    25.000000  \n",
       "max     808.156251                 3.000000             3.000000    63.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.save']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# After fitting the scaler:\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler to a file called \"scaler.save\"\n",
    "joblib.dump(scaler, 'scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 08m 03s]\n",
      "val_loss: 4.436497449874878\n",
      "\n",
      "Best val_loss So Far: 4.365634441375732\n",
      "Total elapsed time: 09h 58m 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Brain Age\\brain_age_env\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 66 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Model 1: Ultimate Super-Residual Model (Ultra_ResDNN)\n",
    "#####################################\n",
    "def build_super_residual_model(hp):\n",
    "    inputs = Input(shape=(X_train_scaled.shape[1],))\n",
    "    \n",
    "    # Base parameters\n",
    "    base_units = hp.Int('base_units', min_value=128, max_value=512, step=64)\n",
    "    reg = regularizers.l2(hp.Float('l2_reg', 1e-5, 1e-3, sampling='LOG'))\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.2, 0.5, step=0.1)\n",
    "    \n",
    "    # Initial layer\n",
    "    x = Dense(units=base_units, activation='relu', kernel_regularizer=reg)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Multiple residual blocks\n",
    "    num_blocks = hp.Int('num_blocks', min_value=3, max_value=6, step=1)\n",
    "    for i in range(num_blocks):\n",
    "        shortcut = x\n",
    "        x = Dense(units=base_units, activation='relu', kernel_regularizer=reg)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(rate=dropout_rate)(x)\n",
    "        x = Dense(units=base_units, activation='relu', kernel_regularizer=reg)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        # Residual connection (dimensions match)\n",
    "        x = Add()([shortcut, x])\n",
    "    \n",
    "    # Final dense layers after residual blocks\n",
    "    post_units = hp.Int('post_units', min_value=64, max_value=256, step=32)\n",
    "    x = Dense(units=post_units, activation='relu', kernel_regularizer=reg)(x)\n",
    "    x = Dropout(rate=hp.Float('dropout_post', 0.1, 0.4, step=0.1))(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=\"Ultra_ResDNN\")\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='mae', metrics=['mse','mae'])\n",
    "    return model\n",
    "\n",
    "tuner_residual = kt.RandomSearch(\n",
    "    build_super_residual_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=2,\n",
    "    directory='tuner_super_residual',\n",
    "    project_name='brain_age_ultra_residual',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner_residual.search(X_train_scaled, y_train, validation_split=0.2, epochs=500,\n",
    "                        callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "                        verbose=1)\n",
    "best_residual_model = tuner_residual.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 07m 24s]\n",
      "val_loss: 4.4709203243255615\n",
      "\n",
      "Best val_loss So Far: 4.293081998825073\n",
      "Total elapsed time: 05h 41m 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Brain Age\\brain_age_env\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Model 2: Extreme Wide & Deep Neural Network (Ultra_WideDeep)\n",
    "#####################################\n",
    "def build_extreme_wide_deep_model(hp):\n",
    "    inputs = Input(shape=(X_train_scaled.shape[1],))\n",
    "    reg = regularizers.l2(hp.Float('l2_reg', 1e-5, 1e-3, sampling='LOG'))\n",
    "    \n",
    "    # Deep branch: many layers\n",
    "    deep = Dense(units=hp.Int('deep_units1', min_value=256, max_value=1024, step=128), activation='relu', kernel_regularizer=reg)(inputs)\n",
    "    deep = BatchNormalization()(deep)\n",
    "    deep = Dropout(rate=hp.Float('dropout_deep1', 0.2, 0.6, step=0.1))(deep)\n",
    "    deep = Dense(units=hp.Int('deep_units2', min_value=128, max_value=512, step=64), activation='relu', kernel_regularizer=reg)(deep)\n",
    "    deep = BatchNormalization()(deep)\n",
    "    deep = Dropout(rate=hp.Float('dropout_deep2', 0.2, 0.6, step=0.1))(deep)\n",
    "    deep = Dense(units=hp.Int('deep_units3', min_value=64, max_value=256, step=32), activation='relu', kernel_regularizer=reg)(deep)\n",
    "    \n",
    "    # Wide branch: direct connection (linear)\n",
    "    wide = Dense(1, activation='linear')(inputs)\n",
    "    \n",
    "    # Combine deep and wide\n",
    "    combined = Concatenate()([deep, wide])\n",
    "    combined = Dense(units=hp.Int('combined_units', min_value=128, max_value=512, step=64), activation='relu', kernel_regularizer=reg)(combined)\n",
    "    combined = Dropout(rate=hp.Float('dropout_combined', 0.1, 0.5, step=0.1))(combined)\n",
    "    outputs = Dense(1)(combined)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=\"Ultra_WideDeep\")\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='mae', metrics=['mse','mae'])\n",
    "    return model\n",
    "\n",
    "tuner_wide_deep = kt.RandomSearch(\n",
    "    build_extreme_wide_deep_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=2,\n",
    "    directory='tuner_extreme_wide_deep',\n",
    "    project_name='brain_age_ultra_wide_deep',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner_wide_deep.search(X_train_scaled, y_train, validation_split=0.2, epochs=500,\n",
    "                         callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "                         verbose=1)\n",
    "best_wide_deep_model = tuner_wide_deep.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 07m 12s]\n",
      "val_loss: 4.437074184417725\n",
      "\n",
      "Best val_loss So Far: 4.287467002868652\n",
      "Total elapsed time: 05h 18m 58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Brain Age\\brain_age_env\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Model 3: Cutting-Edge Attention Neural Network (Ultra_Attention)\n",
    "#####################################\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Reshape\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_advanced_attention_model(hp):\n",
    "    inputs = Input(shape=(X_train_scaled.shape[1],))\n",
    "    reg = regularizers.l2(hp.Float('l2_reg', 1e-5, 1e-3, sampling='LOG'))\n",
    "    \n",
    "    # Determine projection dimension and compute total units needed for reshaping.\n",
    "    proj_dim = hp.Int('proj_dim', min_value=32, max_value=128, step=16)\n",
    "    num_features = X_train_scaled.shape[1]  # should be 4 in this case\n",
    "    total_units = num_features * proj_dim  # must match the total elements in the target shape\n",
    "\n",
    "    # Project inputs to a vector of length total_units.\n",
    "    x_proj = Dense(total_units, activation='relu', kernel_regularizer=reg)(inputs)\n",
    "    \n",
    "    # Reshape to (batch_size, num_features, proj_dim)\n",
    "    x_reshaped = Reshape((num_features, proj_dim))(x_proj)\n",
    "    \n",
    "    # Multi-Head Attention layer\n",
    "    num_heads = hp.Int('num_heads', min_value=2, max_value=4, step=1)\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=proj_dim//num_heads)(x_reshaped, x_reshaped)\n",
    "    \n",
    "    # Flatten the output from multi-head attention\n",
    "    attn_flat = tf.keras.layers.Flatten()(attn_output)\n",
    "    \n",
    "    # Deep layers after attention\n",
    "    x = Dense(units=hp.Int('dense_units1', min_value=256, max_value=1024, step=128),\n",
    "              activation='relu', kernel_regularizer=reg)(attn_flat)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=hp.Float('dropout_att1', 0.2, 0.6, step=0.1))(x)\n",
    "    x = Dense(units=hp.Int('dense_units2', min_value=128, max_value=512, step=64),\n",
    "              activation='relu', kernel_regularizer=reg)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=\"Ultra_Attention\")\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='mae', metrics=['mse','mae'])\n",
    "    return model\n",
    "\n",
    "tuner_attention = kt.RandomSearch(\n",
    "    build_advanced_attention_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=2,\n",
    "    directory='tuner_advanced_attention',\n",
    "    project_name='brain_age_ultra_attention',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner_attention.search(X_train_scaled, y_train, validation_split=0.2, epochs=500,\n",
    "                         callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "                         verbose=1)\n",
    "best_attention_model = tuner_attention.get_best_models(num_models=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - loss: 4.1790 - mae: 4.1247 - mse: 48.2026 - val_loss: 4.3849 - val_mae: 4.3311 - val_mse: 51.7757\n",
      "Epoch 2/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 4.3190 - mae: 4.2653 - mse: 54.1329 - val_loss: 4.3637 - val_mae: 4.3099 - val_mse: 55.2245\n",
      "Epoch 3/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 4.1015 - mae: 4.0478 - mse: 49.4096 - val_loss: 4.4143 - val_mae: 4.3605 - val_mse: 51.9728\n",
      "Epoch 4/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 4.3075 - mae: 4.2537 - mse: 54.5660 - val_loss: 4.4291 - val_mae: 4.3751 - val_mse: 53.0795\n",
      "Epoch 5/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.2428 - mae: 4.1888 - mse: 53.1459 - val_loss: 4.4474 - val_mae: 4.3932 - val_mse: 53.0444\n",
      "Epoch 6/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.1795 - mae: 4.1253 - mse: 50.8165 - val_loss: 4.6100 - val_mae: 4.5557 - val_mse: 57.2700\n",
      "Epoch 7/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4.2625 - mae: 4.2082 - mse: 53.1512 - val_loss: 4.4076 - val_mae: 4.3532 - val_mse: 53.2709\n",
      "Epoch 8/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.0090 - mae: 3.9546 - mse: 46.2919 - val_loss: 4.3776 - val_mae: 4.3231 - val_mse: 51.5270\n",
      "Epoch 9/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1115 - mae: 4.0569 - mse: 45.5404 - val_loss: 4.3031 - val_mae: 4.2484 - val_mse: 51.1054\n",
      "Epoch 10/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1547 - mae: 4.1001 - mse: 49.9986 - val_loss: 4.4240 - val_mae: 4.3694 - val_mse: 54.4078\n",
      "Epoch 11/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.2010 - mae: 4.1463 - mse: 51.5400 - val_loss: 4.4783 - val_mae: 4.4236 - val_mse: 56.5447\n",
      "Epoch 12/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.2271 - mae: 4.1723 - mse: 51.0075 - val_loss: 4.4387 - val_mae: 4.3837 - val_mse: 53.0594\n",
      "Epoch 13/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.2188 - mae: 4.1637 - mse: 53.3710 - val_loss: 4.5260 - val_mae: 4.4708 - val_mse: 56.6812\n",
      "Epoch 14/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 4.1150 - mae: 4.0597 - mse: 48.9955 - val_loss: 4.4624 - val_mae: 4.4069 - val_mse: 56.4662\n",
      "Epoch 15/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 4.3665 - mae: 4.3111 - mse: 55.7311 - val_loss: 4.3594 - val_mae: 4.3039 - val_mse: 52.5979\n",
      "Epoch 16/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.2997 - mae: 4.2442 - mse: 54.2110 - val_loss: 4.4526 - val_mae: 4.3970 - val_mse: 55.6083\n",
      "Epoch 17/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.3715 - mae: 4.3159 - mse: 54.3070 - val_loss: 4.4019 - val_mae: 4.3463 - val_mse: 51.4247\n",
      "Epoch 18/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3.9730 - mae: 3.9174 - mse: 46.4854 - val_loss: 4.3839 - val_mae: 4.3282 - val_mse: 51.8521\n",
      "Epoch 19/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.1945 - mae: 4.1388 - mse: 52.7765 - val_loss: 4.3638 - val_mae: 4.3081 - val_mse: 54.0211\n",
      "Epoch 20/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.2918 - mae: 4.2360 - mse: 54.4026 - val_loss: 4.4344 - val_mae: 4.3785 - val_mse: 56.8408\n",
      "Epoch 21/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1857 - mae: 4.1298 - mse: 53.3432 - val_loss: 4.3521 - val_mae: 4.2962 - val_mse: 52.2385\n",
      "Epoch 22/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 4.2163 - mae: 4.1603 - mse: 53.7249 - val_loss: 4.4353 - val_mae: 4.3792 - val_mse: 55.8428\n",
      "Epoch 23/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 4.2721 - mae: 4.2160 - mse: 54.9473 - val_loss: 4.3933 - val_mae: 4.3371 - val_mse: 55.6773\n",
      "Epoch 24/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.1560 - mae: 4.0998 - mse: 49.4399 - val_loss: 4.4279 - val_mae: 4.3715 - val_mse: 54.1761\n",
      "Epoch 25/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.2974 - mae: 4.2410 - mse: 55.8206 - val_loss: 4.3577 - val_mae: 4.3013 - val_mse: 53.1458\n",
      "Epoch 26/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 4.1308 - mae: 4.0743 - mse: 49.8116 - val_loss: 4.4139 - val_mae: 4.3573 - val_mse: 53.4928\n",
      "Epoch 27/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.4785 - mae: 4.4219 - mse: 59.2547 - val_loss: 4.4432 - val_mae: 4.3864 - val_mse: 54.3327\n",
      "Epoch 28/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.1886 - mae: 4.1317 - mse: 50.0642 - val_loss: 4.3967 - val_mae: 4.3398 - val_mse: 56.6776\n",
      "Epoch 29/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1617 - mae: 4.1047 - mse: 50.4306 - val_loss: 4.4044 - val_mae: 4.3474 - val_mse: 51.5652\n",
      "Epoch 30/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1750 - mae: 4.1181 - mse: 51.7374 - val_loss: 4.3152 - val_mae: 4.2582 - val_mse: 54.6189\n",
      "Epoch 31/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.2704 - mae: 4.2133 - mse: 54.5084 - val_loss: 4.3607 - val_mae: 4.3036 - val_mse: 51.3554\n",
      "Epoch 32/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 4.1877 - mae: 4.1306 - mse: 47.8639 - val_loss: 4.3984 - val_mae: 4.3413 - val_mse: 51.8472\n",
      "Epoch 33/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.3889 - mae: 4.3317 - mse: 59.2376 - val_loss: 4.3321 - val_mae: 4.2748 - val_mse: 51.1900\n",
      "Epoch 34/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1584 - mae: 4.1011 - mse: 50.5824 - val_loss: 4.3849 - val_mae: 4.3275 - val_mse: 54.8305\n",
      "Epoch 35/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.2016 - mae: 4.1442 - mse: 50.0785 - val_loss: 4.4435 - val_mae: 4.3860 - val_mse: 52.2007\n",
      "Epoch 36/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.0423 - mae: 3.9848 - mse: 46.2870 - val_loss: 4.4083 - val_mae: 4.3508 - val_mse: 52.7103\n",
      "Epoch 37/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.2615 - mae: 4.2040 - mse: 53.8621 - val_loss: 4.4174 - val_mae: 4.3599 - val_mse: 52.6070\n",
      "Epoch 38/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.3282 - mae: 4.2706 - mse: 54.1781 - val_loss: 4.4850 - val_mae: 4.4275 - val_mse: 54.7709\n",
      "Epoch 39/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1930 - mae: 4.1355 - mse: 51.4514 - val_loss: 4.4478 - val_mae: 4.3902 - val_mse: 56.4299\n",
      "Epoch 40/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 4.1149 - mae: 4.0572 - mse: 46.5844 - val_loss: 4.3976 - val_mae: 4.3398 - val_mse: 52.5842\n",
      "Epoch 41/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.3530 - mae: 4.2953 - mse: 57.3925 - val_loss: 4.4290 - val_mae: 4.3712 - val_mse: 54.2863\n",
      "Epoch 42/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.3479 - mae: 4.2902 - mse: 56.8622 - val_loss: 4.4488 - val_mae: 4.3909 - val_mse: 55.7322\n",
      "Epoch 43/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 4.4520 - mae: 4.3941 - mse: 59.5647 - val_loss: 4.3127 - val_mae: 4.2547 - val_mse: 51.4911\n",
      "Epoch 44/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.2762 - mae: 4.2181 - mse: 54.2057 - val_loss: 4.3321 - val_mae: 4.2740 - val_mse: 49.9556\n",
      "Epoch 45/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.3388 - mae: 4.2807 - mse: 57.2005 - val_loss: 4.3879 - val_mae: 4.3297 - val_mse: 51.8984\n",
      "Epoch 46/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1133 - mae: 4.0551 - mse: 49.6629 - val_loss: 4.3470 - val_mae: 4.2887 - val_mse: 52.1276\n",
      "Epoch 47/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.2246 - mae: 4.1663 - mse: 53.2572 - val_loss: 4.3618 - val_mae: 4.3034 - val_mse: 50.9496\n",
      "Epoch 48/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 4.0876 - mae: 4.0292 - mse: 46.3478 - val_loss: 4.3658 - val_mae: 4.3073 - val_mse: 52.3343\n",
      "Epoch 49/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4.2679 - mae: 4.2094 - mse: 53.8413 - val_loss: 4.4279 - val_mae: 4.3693 - val_mse: 53.6929\n",
      "Epoch 50/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4.1918 - mae: 4.1332 - mse: 52.2790 - val_loss: 4.4058 - val_mae: 4.3470 - val_mse: 55.5943\n",
      "Epoch 51/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1743 - mae: 4.1155 - mse: 48.4750 - val_loss: 4.3774 - val_mae: 4.3186 - val_mse: 51.6224\n",
      "Epoch 52/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.1835 - mae: 4.1247 - mse: 46.6299 - val_loss: 4.4222 - val_mae: 4.3633 - val_mse: 54.9027\n",
      "Epoch 53/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4.1685 - mae: 4.1095 - mse: 48.2517 - val_loss: 4.4721 - val_mae: 4.4131 - val_mse: 54.4516\n",
      "Epoch 54/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.2953 - mae: 4.2363 - mse: 52.1581 - val_loss: 4.4602 - val_mae: 4.4011 - val_mse: 55.7382\n",
      "Epoch 55/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4.3430 - mae: 4.2838 - mse: 54.3102 - val_loss: 4.4629 - val_mae: 4.4037 - val_mse: 56.3592\n",
      "Epoch 56/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 4.1988 - mae: 4.1395 - mse: 51.7146 - val_loss: 4.4120 - val_mae: 4.3527 - val_mse: 55.5271\n",
      "Epoch 57/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 4.2528 - mae: 4.1934 - mse: 53.1665 - val_loss: 4.3872 - val_mae: 4.3278 - val_mse: 54.8475\n",
      "Epoch 58/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.3129 - mae: 4.2535 - mse: 55.5683 - val_loss: 4.4335 - val_mae: 4.3740 - val_mse: 53.6944\n",
      "Epoch 59/500\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 4.1875 - mae: 4.1279 - mse: 51.1085 - val_loss: 4.4297 - val_mae: 4.3700 - val_mse: 53.7844\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Ultimate Ensemble Model: Stacking/Averaging the Predictions\n",
    "#####################################\n",
    "ensemble_input = Input(shape=(X_train_scaled.shape[1],))\n",
    "out1 = best_residual_model(ensemble_input)\n",
    "out2 = best_wide_deep_model(ensemble_input)\n",
    "out3 = best_attention_model(ensemble_input)\n",
    "ensemble_output = Average()([out1, out2, out3])\n",
    "ensemble_model = Model(ensemble_input, ensemble_output, name=\"Ultimate_Ensemble_Model\")\n",
    "\n",
    "ensemble_model.compile(optimizer=Adam(learning_rate=0.001), loss='mae', metrics=['mse','mae'])\n",
    "\n",
    "# Fine-tune the ensemble on training data\n",
    "ensemble_early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "ensemble_history = ensemble_model.fit(X_train_scaled, y_train,\n",
    "                                      validation_split=0.2,\n",
    "                                      epochs=500,\n",
    "                                      batch_size=32,\n",
    "                                      callbacks=[ensemble_early_stop],\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.208929538726807\n",
      "compile_metrics: 47.553428649902344\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "r²: 0.9638542679790925\n",
      "Final ensemble model saved as Ultimate_Brain_Model.keras\n"
     ]
    }
   ],
   "source": [
    "results = ensemble_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "for name, value in zip(ensemble_model.metrics_names, results):\n",
    "    print(f\"{name}: {value}\")\n",
    "    \n",
    "predictions = ensemble_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute R² using scikit-learn\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"r²:\", r2)\n",
    "\n",
    "# Save final ensemble model in the new Keras format\n",
    "if os.path.exists(\"Ultimate_Brain_Model.keras\"):\n",
    "    os.remove(\"Ultimate_Brain_Model.keras\")\n",
    "ensemble_model.save(\"Ultimate_Brain_Model.keras\")\n",
    "print(\"Final ensemble model saved as Ultimate_Brain_Model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_age_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
